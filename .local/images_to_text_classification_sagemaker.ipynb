{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import torch\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import pipeline\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 5807\n",
      "Example image paths:\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-0CTxYw82SWnJfzPOBBIOQ.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-0fa0mOVKrJW90MFFxVImg.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-4PTjFxdyR-tkxDhVeuAfQ.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-6wM47iMcw_wjW3gZYaz-g.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-7Z1mIroHNK6IJKHMLfnJg.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"sagemaker-studio-619071335465-8h7owh9eftx\"\n",
    "main_image_dir = 'image datasets/'\n",
    "\n",
    "\n",
    "def get_all_images(bucket, prefix):\n",
    "    continuation_token = None\n",
    "    image_keys = []\n",
    "\n",
    "    while True:\n",
    "        list_params = {\n",
    "            'Bucket': bucket,\n",
    "            'Prefix': prefix,\n",
    "        }\n",
    "        if continuation_token:\n",
    "            list_params['ContinuationToken'] = continuation_token\n",
    "\n",
    "        response = s3.list_objects_v2(**list_params)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.jpg'):\n",
    "                    image_keys.append(key)\n",
    "\n",
    "        if not response.get('IsTruncated'):\n",
    "            break\n",
    "\n",
    "        continuation_token = response.get('NextContinuationToken')\n",
    "\n",
    "    return image_keys\n",
    "\n",
    "\n",
    "image_paths = get_all_images(bucket_name, main_image_dir)\n",
    "\n",
    "\n",
    "if len(image_paths) > 0:\n",
    "    print(f\"Total images found: {len(image_paths)}\")\n",
    "    print(\"Example image paths:\")\n",
    "    for path in image_paths[:5]:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images so far. Elapsed time: 146.15 seconds.\n",
      "Processed 200 images so far. Elapsed time: 288.40 seconds.\n",
      "Processed 300 images so far. Elapsed time: 431.32 seconds.\n",
      "Processed 400 images so far. Elapsed time: 575.35 seconds.\n",
      "Processed 500 images so far. Elapsed time: 721.07 seconds.\n",
      "Processed 600 images so far. Elapsed time: 866.12 seconds.\n",
      "Processed 700 images so far. Elapsed time: 1012.02 seconds.\n",
      "Processed 800 images so far. Elapsed time: 1155.97 seconds.\n",
      "Processed 900 images so far. Elapsed time: 1299.35 seconds.\n",
      "Processed 1000 images so far. Elapsed time: 1441.80 seconds.\n",
      "Processed 1100 images so far. Elapsed time: 1586.78 seconds.\n",
      "Processed 1200 images so far. Elapsed time: 1731.87 seconds.\n",
      "Processed 1300 images so far. Elapsed time: 1876.10 seconds.\n",
      "Processed 1400 images so far. Elapsed time: 2019.75 seconds.\n",
      "Processed 1500 images so far. Elapsed time: 2163.18 seconds.\n",
      "Processed 1600 images so far. Elapsed time: 2305.31 seconds.\n",
      "Processed 1700 images so far. Elapsed time: 2447.39 seconds.\n",
      "Processed 1800 images so far. Elapsed time: 2592.25 seconds.\n",
      "Processed 1900 images so far. Elapsed time: 2738.79 seconds.\n",
      "Processed 2000 images so far. Elapsed time: 2885.79 seconds.\n",
      "Processed 2100 images so far. Elapsed time: 3031.98 seconds.\n",
      "Processed 2200 images so far. Elapsed time: 3177.40 seconds.\n",
      "Processed 2300 images so far. Elapsed time: 3326.14 seconds.\n",
      "Processed 2400 images so far. Elapsed time: 3468.69 seconds.\n",
      "Processed 2500 images so far. Elapsed time: 3613.57 seconds.\n",
      "Processed 2600 images so far. Elapsed time: 3756.08 seconds.\n",
      "Processed 2700 images so far. Elapsed time: 3901.82 seconds.\n",
      "Processed 2800 images so far. Elapsed time: 4046.85 seconds.\n",
      "Processed 2900 images so far. Elapsed time: 4195.51 seconds.\n",
      "Processed 3000 images so far. Elapsed time: 4343.27 seconds.\n",
      "Processed 3100 images so far. Elapsed time: 4489.06 seconds.\n",
      "Processed 3200 images so far. Elapsed time: 4637.19 seconds.\n",
      "Processed 3300 images so far. Elapsed time: 4781.15 seconds.\n",
      "Processed 3400 images so far. Elapsed time: 4924.06 seconds.\n",
      "Processed 3500 images so far. Elapsed time: 5066.35 seconds.\n",
      "Processed 3600 images so far. Elapsed time: 5210.89 seconds.\n",
      "Processed 3700 images so far. Elapsed time: 5357.08 seconds.\n",
      "Processed 3800 images so far. Elapsed time: 5507.21 seconds.\n",
      "Processed 3900 images so far. Elapsed time: 5652.90 seconds.\n",
      "Processed 4000 images so far. Elapsed time: 5798.17 seconds.\n",
      "Processed 4100 images so far. Elapsed time: 5945.57 seconds.\n",
      "Processed 4200 images so far. Elapsed time: 6092.00 seconds.\n",
      "Processed 4300 images so far. Elapsed time: 6236.72 seconds.\n",
      "Processed 4400 images so far. Elapsed time: 6378.76 seconds.\n",
      "Processed 4500 images so far. Elapsed time: 6524.03 seconds.\n",
      "Processed 4600 images so far. Elapsed time: 6667.99 seconds.\n",
      "Processed 4700 images so far. Elapsed time: 6812.12 seconds.\n",
      "Processed 4800 images so far. Elapsed time: 6956.21 seconds.\n",
      "Processed 4900 images so far. Elapsed time: 7102.90 seconds.\n",
      "Processed 5000 images so far. Elapsed time: 7248.42 seconds.\n",
      "Processed 5100 images so far. Elapsed time: 7389.91 seconds.\n",
      "Processed 5200 images so far. Elapsed time: 7533.05 seconds.\n",
      "Processed 5300 images so far. Elapsed time: 7675.45 seconds.\n",
      "Processed 5400 images so far. Elapsed time: 7819.76 seconds.\n",
      "Processed 5500 images so far. Elapsed time: 7966.25 seconds.\n",
      "Processed 5600 images so far. Elapsed time: 8111.59 seconds.\n",
      "Processed 5700 images so far. Elapsed time: 8255.12 seconds.\n",
      "Processed 5800 images so far. Elapsed time: 8398.45 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "models_to_test = [\n",
    "    {\"type\": \"pipeline\", \"model\": \"nlpconnect/vit-gpt2-image-captioning\"},\n",
    "    {\"type\": \"blip\", \"model\": \"Salesforce/blip-image-captioning-large\"},\n",
    "    {\"type\": \"blip\", \"model\": \"Salesforce/blip-image-captioning-base\"}\n",
    "]\n",
    "\n",
    "\n",
    "loaded_models = []\n",
    "\n",
    "for model_info in models_to_test:\n",
    "    if model_info[\"type\"] == \"pipeline\":\n",
    "        loaded_models.append({\n",
    "            \"type\": \"pipeline\",\n",
    "            \"model_name\": model_info[\"model\"],\n",
    "            \"model\": pipeline(\"image-to-text\", model=model_info[\"model\"])\n",
    "        })\n",
    "    elif model_info[\"type\"] == \"blip\":\n",
    "        processor = BlipProcessor.from_pretrained(model_info[\"model\"])\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_info[\"model\"]).to(\"cuda\")\n",
    "        loaded_models.append({\n",
    "            \"type\": \"blip\",\n",
    "            \"model_name\": model_info[\"model\"],\n",
    "            \"processor\": processor,\n",
    "            \"model\": model\n",
    "        })\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "def load_image_from_s3(s3_key):\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "    image_data = response['Body'].read()\n",
    "    image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def caption_image_with_models_s3(s3_key):\n",
    "    raw_image = load_image_from_s3(s3_key)\n",
    "    image_id = os.path.basename(s3_key)\n",
    "\n",
    "    for model_data in loaded_models:\n",
    "        if model_data[\"type\"] == \"pipeline\":\n",
    "            result = model_data[\"model\"](raw_image)\n",
    "            caption_text = result[0]['generated_text']\n",
    "        elif model_data[\"type\"] == \"blip\":\n",
    "            processor = model_data[\"processor\"]\n",
    "            model = model_data[\"model\"]\n",
    "\n",
    "            inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "            out = model.generate(**inputs)\n",
    "            caption_text = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        results.append({\n",
    "            \"photo_id\": image_id,\n",
    "            \"model_name\": model_data[\"model_name\"],\n",
    "            \"caption\": caption_text\n",
    "        })\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, s3_key in enumerate(image_paths, start=1):\n",
    "    caption_image_with_models_s3(s3_key)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i} images so far. Elapsed time: {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17421, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>nlpconnect/vit-gpt2-image-captioning</td>\n",
       "      <td>a refrigerator with a picture of a pizza on it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-large</td>\n",
       "      <td>there is a large salad bar with a bunch of veg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-base</td>\n",
       "      <td>a kitchen with a large sign above it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0fa0mOVKrJW90MFFxVImg.jpg</td>\n",
       "      <td>nlpconnect/vit-gpt2-image-captioning</td>\n",
       "      <td>a hot dog with mustard and ketchup on a bun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0fa0mOVKrJW90MFFxVImg.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-large</td>\n",
       "      <td>araffe with a pickle and a side of french fries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     photo_id                              model_name  \\\n",
       "0  -0CTxYw82SWnJfzPOBBIOQ.jpg    nlpconnect/vit-gpt2-image-captioning   \n",
       "1  -0CTxYw82SWnJfzPOBBIOQ.jpg  Salesforce/blip-image-captioning-large   \n",
       "2  -0CTxYw82SWnJfzPOBBIOQ.jpg   Salesforce/blip-image-captioning-base   \n",
       "3  -0fa0mOVKrJW90MFFxVImg.jpg    nlpconnect/vit-gpt2-image-captioning   \n",
       "4  -0fa0mOVKrJW90MFFxVImg.jpg  Salesforce/blip-image-captioning-large   \n",
       "\n",
       "                                             caption  \n",
       "0    a refrigerator with a picture of a pizza on it   \n",
       "1  there is a large salad bar with a bunch of veg...  \n",
       "2               a kitchen with a large sign above it  \n",
       "3       a hot dog with mustard and ketchup on a bun   \n",
       "4    araffe with a pickle and a side of french fries  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images processed. Results saved to 'final_image_to_text_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "df_results.to_csv(\"final_image_to_text_results.csv\", index=False)\n",
    "s3.upload_file('final_image_to_text_results.csv', Bucket=bucket_name, Key='training/image classification/final_image_to_text_results.csv')\n",
    "print(\"All images processed. Results saved to 'final_image_to_text_results.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
