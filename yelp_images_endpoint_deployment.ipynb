{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 03:57:40.587885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pipelines.preprocessing_images import extract_features_from_images_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features, text_features, labels, restaurants, photo_ids = extract_features_from_images_text(2, \n",
    "\"sagemaker-studio-619071335465-8h7owh9eftx\",\n",
    "'samples/image classification/',\n",
    "'samples/image classification/final_image_to_text_results_other.csv',\n",
    "'image datasets/other images/',\n",
    "\"/home/sagemaker-user/clip_model\",\n",
    "\"/home/sagemaker-user/clip_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[ 4.1057e-03,  8.4955e-04, -3.3628e-03,  ..., -2.6028e-03,\n",
      "          7.6136e-04, -2.7815e-04],\n",
      "        [-2.7187e-03, -1.8087e-03,  2.2379e-03,  ...,  4.7140e-03,\n",
      "          1.0096e-03,  1.6289e-04],\n",
      "        [ 3.5197e-03,  5.2734e-03, -6.2515e-03,  ..., -2.2891e-03,\n",
      "         -1.6100e-03, -2.6531e-03],\n",
      "        ...,\n",
      "        [ 5.7256e-03,  3.6784e-03, -5.2710e-03,  ..., -3.3767e-03,\n",
      "         -1.5594e-04, -2.4984e-03],\n",
      "        [-3.9317e-03, -5.1555e-05,  4.2796e-03,  ...,  2.0974e-03,\n",
      "         -7.9344e-04,  6.9403e-04],\n",
      "        [ 9.2486e-44, -8.5479e-44, -7.9874e-44,  ..., -4.6243e-44,\n",
      "          8.8282e-44,  9.6690e-44]])), ('fc1.bias', tensor([ 6.6227e-03, -5.1327e-04,  6.2926e-03,  2.0024e-20,  4.5280e-03,\n",
      "         5.4196e-03,  6.8448e-03, -2.5870e-17,  6.0964e-03,  3.3327e-03,\n",
      "         5.8031e-03, -1.2752e-03,  4.5492e-03, -1.6060e-03,  1.1419e-05,\n",
      "         5.7791e-03,  3.4429e-03,  4.7352e-03, -1.9316e-20, -3.1827e-04,\n",
      "         5.1584e-03, -1.2274e-03, -1.2458e-03, -5.1486e-17, -4.2039e-44,\n",
      "         1.7170e-20, -7.8473e-44,  5.4703e-03, -3.0712e-04,  6.4722e-11,\n",
      "        -7.9874e-44, -7.9874e-44,  2.7546e-03, -6.2217e-04,  5.7192e-03,\n",
      "         1.2292e-03,  1.4863e-10,  3.9519e-03, -2.5846e-05,  4.0305e-03,\n",
      "         4.6382e-03, -8.1275e-44,  4.2144e-03,  4.9919e-03, -5.7789e-05,\n",
      "         2.3883e-03, -6.7985e-04,  5.2187e-03, -4.2311e-14,  5.6202e-03,\n",
      "         6.6962e-03,  9.0571e-17, -7.8473e-44,  2.9500e-03,  3.6539e-03,\n",
      "         6.3006e-03,  1.3473e-03,  3.7095e-03,  5.7182e-03,  5.0876e-03,\n",
      "        -2.8026e-45, -8.4078e-44, -1.4644e-03, -9.5421e-17,  3.9314e-03,\n",
      "        -4.0638e-44, -3.1553e-07, -1.9646e-33,  7.7071e-44,  2.1926e-04,\n",
      "         5.2865e-03,  5.5882e-03, -1.7798e-03, -3.3395e-23,  5.4242e-03,\n",
      "        -7.8473e-44, -8.6452e-04,  5.0827e-03, -8.5173e-22, -9.5288e-44,\n",
      "         1.4071e-03,  4.4104e-03,  4.0551e-03,  5.1674e-03,  5.3760e-03,\n",
      "        -6.2552e-04, -1.4564e-13,  6.5309e-03, -4.0704e-16,  3.6467e-03,\n",
      "         6.7513e-03,  3.0787e-03,  4.5186e-03,  4.0812e-03, -8.3504e-22,\n",
      "         6.8202e-04, -3.0829e-44,  4.6543e-03, -1.4195e-03,  3.2494e-03,\n",
      "         6.2212e-03,  5.9491e-03,  2.1619e-03,  3.7654e-13,  3.6691e-04,\n",
      "        -7.8473e-44,  4.5635e-04,  3.9160e-03, -1.2735e-07, -1.0180e-03,\n",
      "         5.0680e-03, -2.0294e-06,  2.3403e-03, -6.6574e-19,  5.1226e-03,\n",
      "        -9.5863e-08,  3.6851e-03,  3.3211e-03,  3.3571e-05, -7.7071e-44,\n",
      "        -7.7071e-44,  3.3254e-04, -7.1257e-04,  2.9324e-03,  4.3440e-44,\n",
      "        -1.9078e-03,  4.7527e-03,  4.4916e-03,  4.9560e-03, -2.3105e-10,\n",
      "         7.5619e-04, -8.9416e-04, -8.1275e-44,  5.9454e-03, -3.1165e-08,\n",
      "        -7.7071e-44,  3.6017e-03,  5.8916e-03,  2.4435e-30,  1.7507e-20,\n",
      "        -7.7071e-44, -7.8473e-44,  7.6613e-03,  3.0139e-32, -8.1275e-44,\n",
      "         4.0481e-08,  5.7178e-03,  7.4354e-03, -7.8473e-44,  4.3052e-03,\n",
      "         3.0642e-03, -1.5473e-03, -8.1275e-44, -7.8473e-44,  4.1105e-03,\n",
      "         7.2666e-03,  3.0401e-03, -9.1084e-44, -1.0666e-03, -2.2122e-06,\n",
      "         4.9869e-03,  2.1728e-03,  4.4798e-03,  6.4995e-04,  4.7801e-03,\n",
      "         5.3293e-03, -2.8603e-08,  5.5378e-03,  4.4959e-03, -7.8473e-44,\n",
      "        -3.0257e-04,  3.2953e-03,  4.0299e-03, -8.1275e-44, -7.8473e-44,\n",
      "         7.1857e-03, -9.7013e-04,  1.1870e-03,  8.3556e-03, -4.6728e-05,\n",
      "        -7.0065e-45,  4.4192e-03, -7.4066e-04, -6.2645e-04,  6.1995e-03,\n",
      "        -2.0915e-04, -7.8473e-44, -7.8473e-44,  3.2234e-03,  6.3074e-03,\n",
      "        -2.4681e-07,  9.8593e-04, -8.1275e-44,  4.6202e-03, -7.9874e-44,\n",
      "         1.3083e-04,  4.1073e-03, -7.8473e-44,  3.8698e-03,  1.9012e-04,\n",
      "        -1.1392e-03,  3.9057e-03,  3.1105e-03,  4.1451e-03,  5.8091e-03,\n",
      "         3.7214e-03, -1.2732e-03,  6.1460e-04, -7.0065e-44, -4.0946e-04,\n",
      "         4.8076e-03,  3.2065e-03, -1.5877e-03,  5.0784e-04,  7.0838e-04,\n",
      "         5.4041e-03,  5.4691e-03, -1.1816e-03,  7.5748e-03, -8.1275e-44,\n",
      "         3.9236e-44,  2.9427e-44, -1.8079e-03,  7.7071e-44, -7.9874e-44,\n",
      "        -6.8302e-04, -7.7071e-44, -7.8473e-44,  5.3315e-03,  3.6958e-04,\n",
      "        -7.9874e-44,  4.4960e-03,  1.7113e-09,  5.5375e-03, -3.6811e-10,\n",
      "         6.0636e-03,  1.5670e-03, -2.2325e-18, -8.4078e-44, -7.7071e-44,\n",
      "        -9.0113e-04, -2.0323e-03, -7.0065e-45,  1.5304e-09, -4.6716e-18,\n",
      "         3.3157e-03,  4.4348e-03,  4.8238e-03,  7.9086e-03,  2.8884e-03,\n",
      "        -1.3732e-03,  3.7380e-03,  4.1860e-03,  7.3367e-03, -1.3538e-03,\n",
      "        -7.9874e-44])), ('fc2.weight', tensor([[-9.2290e-02,  7.8550e-02, -1.0326e-01,  9.8694e-20, -1.1519e-01,\n",
      "         -1.0135e-01, -1.1149e-01, -9.4784e-16, -8.6607e-02, -3.2183e-02,\n",
      "         -9.5188e-02,  9.5973e-02, -8.3090e-02,  8.9477e-02, -1.7473e-05,\n",
      "         -9.2389e-02, -5.7870e-02, -8.4497e-02, -2.7200e-20,  9.3109e-02,\n",
      "         -1.0946e-01,  8.0058e-02,  5.9261e-02,  3.1818e-16, -7.7071e-44,\n",
      "         -4.6213e-20, -5.4651e-44, -9.5332e-02,  1.0929e-01,  1.1157e-08,\n",
      "          7.8473e-44,  7.8473e-44, -5.3811e-02,  6.3118e-02, -1.1893e-01,\n",
      "         -2.4116e-02,  6.8883e-11, -7.6281e-02,  9.0460e-02, -9.4318e-02,\n",
      "         -9.3344e-02,  7.7071e-44, -9.1204e-02, -6.6589e-02,  7.7995e-02,\n",
      "         -4.6554e-02,  8.2491e-02, -9.7015e-02, -7.2797e-13, -1.1076e-01,\n",
      "         -9.8323e-02, -5.2447e-16,  7.7071e-44, -5.9190e-02, -8.6593e-02,\n",
      "         -1.1417e-01,  5.0785e-02, -8.0169e-02, -1.0628e-01, -8.5536e-02,\n",
      "          7.8473e-44, -7.7071e-44,  9.8501e-02,  1.9890e-15, -1.0251e-01,\n",
      "         -7.7071e-44,  9.9272e-06,  6.1663e-07, -9.3139e-24, -1.0650e-03,\n",
      "         -1.0470e-01, -9.3527e-02,  7.8813e-02,  7.6150e-23, -9.7970e-02,\n",
      "         -7.7071e-44,  5.8199e-02, -1.0237e-01,  2.6513e-21,  7.7071e-44,\n",
      "          8.3118e-02, -8.4802e-02, -7.5312e-02, -1.1380e-01, -7.6661e-02,\n",
      "          8.3750e-02,  1.2496e-12, -1.0394e-01,  1.7629e-15, -7.6808e-02,\n",
      "         -1.1579e-01, -4.5846e-02, -9.1928e-02, -7.1286e-02,  1.1605e-20,\n",
      "          1.0149e-01, -7.7071e-44, -9.5168e-02,  9.6930e-02, -6.9404e-02,\n",
      "         -9.3249e-02, -1.1089e-01, -3.5227e-02,  3.6941e-10,  7.3280e-02,\n",
      "          7.7071e-44,  9.2638e-02, -7.1101e-02,  5.8582e-08,  9.2468e-02,\n",
      "         -1.0800e-01,  2.9077e-05, -2.1788e-02, -1.0100e-18, -9.2355e-02,\n",
      "          3.2350e-07, -8.5526e-02, -7.2064e-02,  3.6913e-02, -7.7071e-44,\n",
      "         -7.7071e-44,  6.3212e-02,  5.6831e-02, -7.2599e-02,  7.7071e-44,\n",
      "          1.0456e-01, -1.0220e-01, -8.1976e-02, -1.0240e-01, -5.9854e-11,\n",
      "          9.2727e-02,  5.5197e-02, -7.7071e-44, -1.0729e-01, -6.8581e-08,\n",
      "         -7.7071e-44, -9.1119e-02, -9.4849e-02,  1.6815e-29, -8.4200e-20,\n",
      "          7.7071e-44, -7.7071e-44, -1.0968e-01,  4.9002e-31, -7.8473e-44,\n",
      "          7.4978e-09, -9.7562e-02, -1.0055e-01,  7.7071e-44, -5.4284e-02,\n",
      "         -6.9652e-02,  9.2817e-02,  7.7071e-44,  7.7071e-44, -8.0447e-02,\n",
      "         -1.0240e-01, -6.1931e-02, -7.8473e-44,  9.7161e-02, -1.6005e-07,\n",
      "         -8.9872e-02,  8.7979e-02, -8.3786e-02,  3.6897e-02, -8.9329e-02,\n",
      "         -9.8979e-02,  2.2817e-07, -9.7315e-02, -1.0289e-01,  7.8473e-44,\n",
      "          8.0877e-02, -5.3698e-02, -7.2802e-02,  7.7071e-44, -7.7071e-44,\n",
      "         -1.0029e-01,  6.2921e-02,  7.3603e-02, -1.0950e-01,  8.9653e-02,\n",
      "          7.7071e-44, -9.2497e-02,  1.0976e-01,  9.1528e-02, -1.2266e-01,\n",
      "          2.9876e-04, -7.7071e-44, -7.7071e-44, -4.5908e-02, -1.0358e-01,\n",
      "         -2.5628e-05,  1.0183e-01,  7.7071e-44, -9.6691e-02, -7.7071e-44,\n",
      "          1.0200e-01, -8.9313e-02, -7.8473e-44, -7.7200e-02,  7.2882e-02,\n",
      "          9.5257e-02, -1.0229e-01, -7.1676e-02, -8.2654e-02, -1.1317e-01,\n",
      "         -8.6684e-02,  8.6963e-02, -6.3915e-03,  7.8473e-44,  1.1508e-01,\n",
      "         -1.0248e-01, -5.8775e-02,  5.3342e-02,  9.5084e-02,  9.7686e-02,\n",
      "         -1.0256e-01, -1.0089e-01,  6.7501e-02, -1.1463e-01,  7.7071e-44,\n",
      "          7.7071e-44, -7.7071e-44,  8.7920e-02, -7.7071e-44,  7.7071e-44,\n",
      "          3.4103e-02, -7.7071e-44,  7.7071e-44, -1.2320e-01,  9.5419e-02,\n",
      "         -7.7071e-44, -6.5799e-02, -7.9352e-09, -1.0073e-01,  7.5798e-08,\n",
      "         -8.8570e-02, -2.6162e-02, -1.7521e-18,  7.7071e-44, -7.7071e-44,\n",
      "          1.0413e-01,  7.0430e-02, -7.7071e-44,  2.7406e-09,  3.4522e-18,\n",
      "         -7.5296e-02, -8.0805e-02, -1.0949e-01, -1.2985e-01, -5.9865e-02,\n",
      "          1.0907e-01, -8.6181e-02, -9.5146e-02, -1.3382e-01,  9.9728e-02,\n",
      "          7.7071e-44]])), ('fc2.bias', tensor([-0.0217]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12857/161036892.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  multimodal_classifier.load_state_dict(torch.load(local_multimodal_model_path, map_location=device))\n",
      "/tmp/ipykernel_12857/161036892.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(local_multimodal_model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 2, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, image_embedding, text_embedding):\n",
    "        x = torch.cat((image_embedding, text_embedding), dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "input_dim = 512\n",
    "hidden_dim = 256\n",
    "dropout_rate = 0.45584099758281393\n",
    "\n",
    "local_multimodal_model_path = \"/home/sagemaker-user/model/multimodal_classifier_model.pth\"\n",
    "\n",
    "\n",
    "multimodal_classifier = MultimodalClassifier(input_dim, hidden_dim, dropout_rate).to(device)\n",
    "multimodal_classifier.load_state_dict(torch.load(local_multimodal_model_path, map_location=device))\n",
    "multimodal_classifier.eval()\n",
    "\n",
    "state_dict = torch.load(local_multimodal_model_path, map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "multimodal_classifier.load_state_dict(state_dict)\n",
    "\n",
    "# Print state_dict\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Restaurant Name: Broussard's\n",
      "Photo ID: 2CmoBjiw0GYOCbakHT3dHA.jpg\n",
      "True Label: Positive\n",
      "Prediction: Positive (Score: 0.5449)\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "Restaurant Name: Biscuit Love: Gulch\n",
      "Photo ID: 5zX5vPMu6gYiPbxvM7jXrQ.jpg\n",
      "True Label: Negative\n",
      "Prediction: Negative (Score: 0.0326)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_multiple(image_embeddings, text_embeddings, labels, restaurants, photo_ids):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    results = []  # Store the combined output for each sample\n",
    "\n",
    "    for i in range(image_embeddings.size(0)):\n",
    "        image_embedding = image_embeddings[i].unsqueeze(0)\n",
    "        text_embedding = text_embeddings[i].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = multimodal_classifier(image_embedding, text_embedding)\n",
    "            prediction_score = prediction.squeeze().item()  \n",
    "            predicted_label = \"Positive\" if prediction_score > 0.5 else \"Negative\"\n",
    "\n",
    "        true_label = \"Positive\" if labels[i].item() == 1 else \"Negative\"\n",
    "        predictions.append((predicted_label, prediction_score))\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "        # Append the current sample's results\n",
    "        results.append({\n",
    "            \"restaurant_name\": restaurants[i],\n",
    "            \"photo_id\": photo_ids[i],\n",
    "            \"true_label\": true_label,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"prediction_score\": prediction_score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = predict_multiple(image_features, text_features, labels, restaurants, photo_ids)\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"Restaurant Name: {result['restaurant_name']}\")\n",
    "    print(f\"Photo ID: {result['photo_id']}\")\n",
    "    print(f\"True Label: {result['true_label']}\")\n",
    "    print(f\"Prediction: {result['predicted_label']} (Score: {result['prediction_score']:.4f})\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar.gz file created at: /home/sagemaker-user/model/multimodal_classifier_model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Paths to your files\n",
    "model_file_path = \"/home/sagemaker-user/model/multimodal_classifier_model.pth\"\n",
    "inference_file_path = \"/home/sagemaker-user/model/inference.py\"\n",
    "\n",
    "# Path to save the tar.gz file\n",
    "tar_file_path = \"/home/sagemaker-user/model/multimodal_classifier_model.tar.gz\"\n",
    "\n",
    "# Create a tar.gz file\n",
    "with tarfile.open(tar_file_path, \"w:gz\") as tar:\n",
    "    # Add the model file to the tar.gz archive\n",
    "    model_name = os.path.basename(model_file_path)  # Extract file name from path\n",
    "    tar.add(model_file_path, arcname=model_name)\n",
    "\n",
    "    # Add the inference.py file to the tar.gz archive\n",
    "    inference_name = os.path.basename(inference_file_path)  # Extract file name from path\n",
    "    tar.add(inference_file_path, arcname=inference_name)\n",
    "\n",
    "print(f\"tar.gz file created at: {tar_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to s3://sagemaker-studio-619071335465-8h7owh9eftx/opt/ml/model/multimodal_classifier_model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3_bucket = \"sagemaker-studio-619071335465-8h7owh9eftx\"\n",
    "s3_key = \"opt/ml/model/multimodal_classifier_model.tar.gz\"\n",
    "\n",
    "# Upload the tar.gz file to S3\n",
    "s3.upload_file(tar_file_path, s3_bucket, s3_key)\n",
    "\n",
    "print(f\"Model uploaded to s3://{s3_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-studio-619071335465-8h7owh9eftx/opt/ml/model/multimodal_classifier_model.tar.gz\n",
      "2.4.1.post100\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "sagemaker_session = Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "model_artifact = f\"s3://{s3_bucket}/{s3_key}\"\n",
    "\n",
    "print(model_artifact)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Model deployed to endpoint: multimodal-classifier-endpoint-0126257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "pytorch_model = PyTorchModel( \n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    framework_version=\"2.1.0\",    # Match your PyTorch version\n",
    "    py_version=\"py310\"\n",
    ")\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "endpoint_name = \"multimodal-classifier-endpoint-0126257\"\n",
    "predictor = pytorch_model.deploy(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: InService\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "endpoint_name = endpoint_name\n",
    "\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint Status: {response['EndpointStatus']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features type: <class 'torch.Tensor'>, dtype: torch.float32\n",
      "text_features type: <class 'torch.Tensor'>, dtype: torch.float32\n",
      "b'[[0.5449047684669495], [0.03256279230117798]]'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import base64\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "print(f\"image_features type: {type(image_features)}, dtype: {image_features.dtype}\")\n",
    "print(f\"text_features type: {type(text_features)}, dtype: {text_features.dtype}\")\n",
    "\n",
    "\n",
    "input_data = {\n",
    "    \"image_embedding\": image_features.tolist(),\n",
    "    \"text_embedding\": text_features.tolist()\n",
    "}\n",
    "\n",
    "response = predictor.predict(\n",
    "    data=json.dumps(input_data),  # Convert your input to JSON format\n",
    "    initial_args={\"ContentType\": \"application/json\"}  # Set Content-Type explicitly\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features type: <class 'torch.Tensor'>, dtype: torch.float32\n",
      "text_features type: <class 'torch.Tensor'>, dtype: torch.float32\n",
      "Response from SageMaker endpoint: [[0.5449047684669495], [0.03256279230117798]]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Define your endpoint name\n",
    "ENDPOINT_NAME = endpoint_name\n",
    "\n",
    "# Assuming you already have image_features and text_features\n",
    "print(f\"image_features type: {type(image_features)}, dtype: {image_features.dtype}\")\n",
    "print(f\"text_features type: {type(text_features)}, dtype: {text_features.dtype}\")\n",
    "\n",
    "# Prepare the payload\n",
    "input_data = {\n",
    "    \"image_embedding\": image_features.tolist(),\n",
    "    \"text_embedding\": text_features.tolist()\n",
    "}\n",
    "\n",
    "# Call the SageMaker endpoint\n",
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(input_data)  # Convert the input to JSON format\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response_payload = response[\"Body\"].read().decode(\"utf-8\")\n",
    "print(\"Response from SageMaker endpoint:\", response_payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(input_data):\n",
    "    # Convert JSON to PyTorch tensors\n",
    "    inputs = json.loads(input_data)\n",
    "    image_embedding = torch.tensor(inputs['image_embedding'], dtype=torch.float32).to(device)\n",
    "    text_embedding = torch.tensor(inputs['text_embedding'], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Verify shapes\n",
    "    print(f\"Image embedding shape: {image_embedding.shape}\")\n",
    "    print(f\"Text embedding shape: {text_embedding.shape}\")\n",
    "\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        # Pass both image_embedding and text_embedding\n",
    "        output = multimodal_classifier(image_embedding, text_embedding)  # Correct pass here\n",
    "        \n",
    "    return output.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: torch.Size([2, 512])\n",
      "Text embedding shape: torch.Size([2, 512])\n",
      "Output: [[0.5449047684669495], [0.03256279230117798]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Specify your endpoint name\n",
    "endpoint_name = ENDPOINT_NAME\n",
    "\n",
    "# Prepare a sample payload\n",
    "# Replace this with actual input your model expects\n",
    "sample_input = {\n",
    "    \"image_embedding\": image_features.tolist(),  # Example embedding\n",
    "    \"text_embedding\": text_features.tolist()   # Example embedding\n",
    "}\n",
    "\n",
    "# Serialize the payload to JSON\n",
    "input_json = json.dumps(sample_input)\n",
    "\n",
    "# print(input_json)\n",
    "\n",
    "output = model_fn(input_json)\n",
    "print(\"Output:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
