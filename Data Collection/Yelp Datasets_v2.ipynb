{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"yelp_academic_dataset_review\" + \".json\"\n",
    "path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "df_review = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i, chunk in enumerate(df_review):\n",
    "    # print(f\"Processing chunk {i+1}\")\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "chunk_review = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "chunk_review.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"yelp_academic_dataset_review_subset\" + \".csv\"\n",
    "out_path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "chunk_review.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"yelp_academic_dataset_user\" + \".json\"\n",
    "path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "df_user = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "user_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(df_user):\n",
    "    # print(f\"Processing chunk {i+1}\")\n",
    "    user_chunks.append(chunk)\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "chunk_user = pd.concat(user_chunks, ignore_index=True)\n",
    "\n",
    "chunk_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"yelp_academic_dataset_user_subset\" + \".csv\"\n",
    "out_path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "chunk_user.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150346, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"yelp_academic_dataset_business\" + \".json\"\n",
    "path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "df_business = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "business_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(df_business):\n",
    "    # print(f\"Processing chunk {i+1}\")\n",
    "    business_chunks.append(chunk)\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "chunk_business = pd.concat(business_chunks, ignore_index=True)\n",
    "\n",
    "chunk_business.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"yelp_academic_dataset_business\" + \".csv\"\n",
    "out_path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "chunk_business.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131930, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"yelp_academic_dataset_checkin\" + \".json\"\n",
    "path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "df_checkin = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "checkin_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(df_checkin):\n",
    "    # print(f\"Processing chunk {i+1}\")\n",
    "    checkin_chunks.append(chunk)\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "chunk_checkin = pd.concat(checkin_chunks, ignore_index=True)\n",
    "\n",
    "chunk_checkin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"yelp_academic_dataset_checkin\" + \".csv\"\n",
    "out_path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "chunk_checkin.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(908915, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"yelp_academic_dataset_tip\" + \".json\"\n",
    "path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "df_tip = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "tip_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(df_tip):\n",
    "    # print(f\"Processing chunk {i+1}\")\n",
    "    tip_chunks.append(chunk)\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "chunk_tip = pd.concat(tip_chunks, ignore_index=True)\n",
    "\n",
    "chunk_tip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"yelp_academic_dataset_tip\" + \".csv\"\n",
    "out_path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "chunk_tip.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 200101\n",
      "Extracted 1000 files\n",
      "Extracted 2000 files\n",
      "Extracted 3000 files\n",
      "Extracted 4000 files\n",
      "Extracted 5000 files\n",
      "Extracted 6000 files\n",
      "Extracted 7000 files\n",
      "Extracted 8000 files\n",
      "Extracted 9000 files\n",
      "Extracted 10000 files\n",
      "Extracted 11000 files\n",
      "Extracted 12000 files\n",
      "Extracted 13000 files\n",
      "Extracted 14000 files\n",
      "Extracted 15000 files\n",
      "Extracted 16000 files\n",
      "Extracted 17000 files\n",
      "Extracted 18000 files\n",
      "Extracted 19000 files\n",
      "Extracted 20000 files\n",
      "Extracted 21000 files\n",
      "Extracted 22000 files\n",
      "Extracted 23000 files\n",
      "Extracted 24000 files\n",
      "Extracted 25000 files\n",
      "Extracted 26000 files\n",
      "Extracted 27000 files\n",
      "Extracted 28000 files\n",
      "Extracted 29000 files\n",
      "Extracted 30000 files\n",
      "Extracted 31000 files\n",
      "Extracted 32000 files\n",
      "Extracted 33000 files\n",
      "Extracted 34000 files\n",
      "Extracted 35000 files\n",
      "Extracted 36000 files\n",
      "Extracted 37000 files\n",
      "Extracted 38000 files\n",
      "Extracted 39000 files\n",
      "Extracted 40000 files\n",
      "Extracted 41000 files\n",
      "Extracted 42000 files\n",
      "Extracted 43000 files\n",
      "Extracted 44000 files\n",
      "Extracted 45000 files\n",
      "Extracted 46000 files\n",
      "Extracted 47000 files\n",
      "Extracted 48000 files\n",
      "Extracted 49000 files\n",
      "Extracted 50000 files\n",
      "Extracted 51000 files\n",
      "Extracted 52000 files\n",
      "Extracted 53000 files\n",
      "Extracted 54000 files\n",
      "Extracted 55000 files\n",
      "Extracted 56000 files\n",
      "Extracted 57000 files\n",
      "Extracted 58000 files\n",
      "Extracted 59000 files\n",
      "Extracted 60000 files\n",
      "Extracted 61000 files\n",
      "Extracted 62000 files\n",
      "Extracted 63000 files\n",
      "Extracted 64000 files\n",
      "Extracted 65000 files\n",
      "Extracted 66000 files\n",
      "Extracted 67000 files\n",
      "Extracted 68000 files\n",
      "Extracted 69000 files\n",
      "Extracted 70000 files\n",
      "Extracted 71000 files\n",
      "Extracted 72000 files\n",
      "Extracted 73000 files\n",
      "Extracted 74000 files\n",
      "Extracted 75000 files\n",
      "Extracted 76000 files\n",
      "Extracted 77000 files\n",
      "Extracted 78000 files\n",
      "Extracted 79000 files\n",
      "Extracted 80000 files\n",
      "Extracted 81000 files\n",
      "Extracted 82000 files\n",
      "Extracted 83000 files\n",
      "Extracted 84000 files\n",
      "Extracted 85000 files\n",
      "Extracted 86000 files\n",
      "Extracted 87000 files\n",
      "Extracted 88000 files\n",
      "Extracted 89000 files\n",
      "Extracted 90000 files\n",
      "Extracted 91000 files\n",
      "Extracted 92000 files\n",
      "Extracted 93000 files\n",
      "Extracted 94000 files\n",
      "Extracted 95000 files\n",
      "Extracted 96000 files\n",
      "Extracted 97000 files\n",
      "Extracted 98000 files\n",
      "Extracted 99000 files\n",
      "Extracted 100000 files\n",
      "Extracted 101000 files\n",
      "Extracted 102000 files\n",
      "Extracted 103000 files\n",
      "Extracted 104000 files\n",
      "Extracted 105000 files\n",
      "Extracted 106000 files\n",
      "Extracted 107000 files\n",
      "Extracted 108000 files\n",
      "Extracted 109000 files\n",
      "Extracted 110000 files\n",
      "Extracted 111000 files\n",
      "Extracted 112000 files\n",
      "Extracted 113000 files\n",
      "Extracted 114000 files\n",
      "Extracted 115000 files\n",
      "Extracted 116000 files\n",
      "Extracted 117000 files\n",
      "Extracted 118000 files\n",
      "Extracted 119000 files\n",
      "Extracted 120000 files\n",
      "Extracted 121000 files\n",
      "Extracted 122000 files\n",
      "Extracted 123000 files\n",
      "Extracted 124000 files\n",
      "Extracted 125000 files\n",
      "Extracted 126000 files\n",
      "Extracted 127000 files\n",
      "Extracted 128000 files\n",
      "Extracted 129000 files\n",
      "Extracted 130000 files\n",
      "Extracted 131000 files\n",
      "Extracted 132000 files\n",
      "Extracted 133000 files\n",
      "Extracted 134000 files\n",
      "Extracted 135000 files\n",
      "Extracted 136000 files\n",
      "Extracted 137000 files\n",
      "Extracted 138000 files\n",
      "Extracted 139000 files\n",
      "Extracted 140000 files\n",
      "Extracted 141000 files\n",
      "Extracted 142000 files\n",
      "Extracted 143000 files\n",
      "Extracted 144000 files\n",
      "Extracted 145000 files\n",
      "Extracted 146000 files\n",
      "Extracted 147000 files\n",
      "Extracted 148000 files\n",
      "Extracted 149000 files\n",
      "Extracted 150000 files\n",
      "Extracted 151000 files\n",
      "Extracted 152000 files\n",
      "Extracted 153000 files\n",
      "Extracted 154000 files\n",
      "Extracted 155000 files\n",
      "Extracted 156000 files\n",
      "Extracted 157000 files\n",
      "Extracted 158000 files\n",
      "Extracted 159000 files\n",
      "Extracted 160000 files\n",
      "Extracted 161000 files\n",
      "Extracted 162000 files\n",
      "Extracted 163000 files\n",
      "Extracted 164000 files\n",
      "Extracted 165000 files\n",
      "Extracted 166000 files\n",
      "Extracted 167000 files\n",
      "Extracted 168000 files\n",
      "Extracted 169000 files\n",
      "Extracted 170000 files\n",
      "Extracted 171000 files\n",
      "Extracted 172000 files\n",
      "Extracted 173000 files\n",
      "Extracted 174000 files\n",
      "Extracted 175000 files\n",
      "Extracted 176000 files\n",
      "Extracted 177000 files\n",
      "Extracted 178000 files\n",
      "Extracted 179000 files\n",
      "Extracted 180000 files\n",
      "Extracted 181000 files\n",
      "Extracted 182000 files\n",
      "Extracted 183000 files\n",
      "Extracted 184000 files\n",
      "Extracted 185000 files\n",
      "Extracted 186000 files\n",
      "Extracted 187000 files\n",
      "Extracted 188000 files\n",
      "Extracted 189000 files\n",
      "Extracted 190000 files\n",
      "Extracted 191000 files\n",
      "Extracted 192000 files\n",
      "Extracted 193000 files\n",
      "Extracted 194000 files\n",
      "Extracted 195000 files\n",
      "Extracted 196000 files\n",
      "Extracted 197000 files\n",
      "Extracted 198000 files\n",
      "Extracted 199000 files\n",
      "Extracted 200000 files\n",
      "Extracted 201000 files\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "tar_file_path = \"C:/Users/tokud/Downloads/yelp_photos.tar\"\n",
    "extract_dir = \"C:/Users/tokud/Downloads/extracted_yelp_photos/photos/photos\"\n",
    "output_dir = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Photos/extracted_images\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "def cleanup_directory(directory):\n",
    "    shutil.rmtree(directory)\n",
    "    os.makedirs(directory) \n",
    "\n",
    "# Function to extract images from the .tar file in chunks\n",
    "def extract_in_chunks(tar_path, extract_to, output_directory, chunk_size=1000):\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = tar.getmembers()  # Get the list of files inside the .tar file\n",
    "        total_files = len(members)\n",
    "        print(f\"Total number of files: {total_files}\")\n",
    "        \n",
    "        for i in range(0, total_files, chunk_size):\n",
    "            chunk = members[i:i + chunk_size]  # Get a chunk of files\n",
    "            tar.extractall(path=extract_to, members=chunk)  # Extract the chunk\n",
    "            print(f\"Extracted {i + chunk_size} files\")\n",
    "            \n",
    "            # Process and display images after each chunk is extracted\n",
    "            process_images_and_display(extract_to, output_directory)\n",
    "            # cleanup_directory(extract_to)\n",
    "            # Optionally break after the first chunk for testing\n",
    "           \n",
    "\n",
    "# Function to process images in chunks from the extracted folder\n",
    "def process_images(directory, chunk_size):\n",
    "    count = 0\n",
    "    image_chunk = []\n",
    "    \n",
    "    for image_file in os.listdir(directory):\n",
    "        if image_file.endswith('.jpg'):\n",
    "            img_path = os.path.join(directory, image_file)\n",
    "            \n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    image_chunk.append((img.copy(), image_file))\n",
    "                count += 1\n",
    "            except UnidentifiedImageError as e:\n",
    "                print(f\"Skipping {img_path}: {e}\")\n",
    "            \n",
    "            if count >= chunk_size:  # When the chunk is filled, yield it\n",
    "                yield image_chunk\n",
    "                image_chunk = []  # Reset the chunk\n",
    "                count = 0\n",
    "    \n",
    "    # Yield any remaining images (if fewer than chunk_size)\n",
    "    if image_chunk:\n",
    "        yield image_chunk\n",
    "        \n",
    "\n",
    "\n",
    "# Function to display the images\n",
    "def display_images(image_list, num_images_to_display=10):\n",
    "    # Select only the first 'num_images_to_display' images\n",
    "    selected_images = [img for img, _ in image_list[:num_images_to_display]]\n",
    "\n",
    "    # Set up the plotting grid\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, img in enumerate(selected_images):\n",
    "        plt.subplot(2, 5, i + 1)  # 2 rows and 5 columns grid\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axes\n",
    "        plt.title(f\"Image {i + 1}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to process and display images after extraction\n",
    "\n",
    "def process_images_and_display(directory, output_directory):\n",
    "    for image_chunk in process_images(directory, chunk_size=1000):\n",
    "        # Display images\n",
    "        # display_images(image_chunk, num_images_to_display=10)\n",
    "\n",
    "        # Save images to the output directory\n",
    "        for img, image_name in image_chunk:\n",
    "            save_path = os.path.join(output_directory, image_name)\n",
    "            img.save(save_path)  # Save the image to the output folder\n",
    "            # print(f\"Saved {image_name} to {save_path}\")\n",
    "        \n",
    "        # break  # Only process the first chunk (you can remove this to process more)\n",
    "\n",
    "\n",
    "# Extract files in chunks of 1000 and process them\n",
    "extract_in_chunks(tar_file_path, extract_dir, output_dir, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_dir = \"C:/Users/tokud/Downloads/extracted_yelp_photos/photos/photos\"\n",
    "output_dir = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Photos/extracted_images\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Iterate through the photos directory and copy all images to the output directory\n",
    "for image_file in os.listdir(photos_dir):\n",
    "    if image_file.endswith('.jpg'):\n",
    "        src_path = os.path.join(photos_dir, image_file)\n",
    "        dest_path = os.path.join(output_dir, image_file)\n",
    "        \n",
    "        shutil.copy(src_path, dest_path)  # Copy the file to the output directory\n",
    "        # print(f\"Copied {image_file} to {dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200100, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"photos\" + \".json\"\n",
    "path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "df_photo = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "photo_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(df_photo):\n",
    "    # print(f\"Processing chunk {i+1}\")\n",
    "    photo_chunks.append(chunk)\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "chunk_photo = pd.concat(photo_chunks, ignore_index=True)\n",
    "\n",
    "chunk_photo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"photos\" + \".csv\"\n",
    "out_path = \"C:/Users/tokud/OneDrive/Documents/Machine Learning - UCSD Extension Springboard Course/Capstone/Datasets/\" + filename\n",
    "\n",
    "chunk_photo.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
