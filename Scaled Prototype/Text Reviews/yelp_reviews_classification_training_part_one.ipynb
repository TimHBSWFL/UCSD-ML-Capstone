{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.3.1.post300)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 22:40:36.779003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-24 22:40:36.799236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-24 22:40:36.805589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 22:40:36.820558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import torch\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: training/text reviews/FL_Reviews_Edited.csv\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"sagemaker-studio-619071335465-8h7owh9eftx\"\n",
    "main_text_dir = 'training/text reviews/'\n",
    "\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=main_text_dir)\n",
    "\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "\n",
    "if len(csv_files) == 1:\n",
    "    csv_file_key = csv_files[0]\n",
    "    print(f\"Found CSV file: {csv_file_key}\")\n",
    "else:\n",
    "    raise ValueError(f\"Expected exactly one CSV file, but found {len(csv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792133, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 10000\n",
    "\n",
    "s3_uri = f\"s3://{bucket_name}/{csv_file_key}\"\n",
    "\n",
    "chunk_list = []\n",
    "\n",
    "for chunk in pd.read_csv(s3_uri, chunksize=chunk_size):\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "df_reviews = pd.concat(chunk_list, ignore_index=True)\n",
    "df_reviews.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474385, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df_reviews[df_reviews['stars_reviews'].isin([1,5])]\n",
    "# df_sample = df_sample.sample(frac=0.0025, random_state=42)\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5517/3190987524.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample['binary_labels'] = df_sample['stars_reviews'].map({1: 0, 5: 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'name', 'address', 'city_original', 'state',\n",
       "       'postal_code', 'latitude', 'longitude', 'stars_business',\n",
       "       'review_count', 'is_open', 'attributes', 'categories', 'hours',\n",
       "       'review_id', 'user_id', 'stars_reviews', 'useful', 'funny', 'cool',\n",
       "       'text', 'date', 'zip_code', 'city_updated', 'binary_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['binary_labels'] = df_sample['stars_reviews'].map({1: 0, 5: 1})\n",
    "df_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_sample['text'].tolist(), df_sample['binary_labels'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars_reviews\n",
       "5    375217\n",
       "1     99168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['stars_reviews'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(distilbert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fcce2d458140e69ff0d2476053bc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/379508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab01a409ac1d4dffb10f04bab83d621b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94877 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 1.0080248949472861e-05\n",
      "batch_size: 64\n",
      "num_epochs: 5\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'eval_accuracy': accuracy\n",
    "    }\n",
    "\n",
    "best_params = {'learning_rate': 1.0080248949472861e-05, 'batch_size': 64, 'num_epochs': 5}\n",
    "\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_num_epochs = best_params['num_epochs']\n",
    "\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming processing from row 0.\n",
      "Processed 1000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 2000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 3000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 4000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 5000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 6000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 7000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 8000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 9000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 10000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 11000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 12000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 13000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 14000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 15000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 16000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 17000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 18000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 19000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 20000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 21000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 22000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 23000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 24000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 25000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 26000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 27000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 28000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 29000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 30000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 31000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 32000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 33000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 34000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 35000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 36000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 37000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 38000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 39000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 40000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 41000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 42000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 43000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 44000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 45000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 46000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 47000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 48000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 49000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 50000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 51000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 52000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 53000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 54000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 55000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 56000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 57000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 58000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 59000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 60000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 61000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 62000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 63000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 64000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 65000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 66000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 67000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 68000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 69000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 70000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 71000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 72000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 73000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 74000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 75000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 76000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 77000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 78000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 79000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 80000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 81000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 82000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 83000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 84000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 85000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 86000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 87000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 88000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 89000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 90000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 91000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 92000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 93000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 94000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 95000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 96000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 97000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 98000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 99000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 100000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 101000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 102000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 103000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 104000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 105000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 106000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 107000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 108000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 109000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 110000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 111000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 112000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 113000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 114000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 115000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 116000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 117000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 118000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 119000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 120000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 121000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 122000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 123000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 124000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 125000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 126000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 127000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 128000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 129000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 130000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 131000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 132000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 133000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 134000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 135000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 136000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 137000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 138000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 139000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 140000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 141000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 142000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 143000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 144000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 145000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 146000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 147000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 148000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 149000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 150000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 151000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 152000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 153000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 154000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 155000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 156000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 157000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 158000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 159000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 160000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 161000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 162000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 163000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 164000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 165000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 166000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 167000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 168000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 169000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 170000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 171000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 172000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 173000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 174000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 175000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 176000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 177000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 178000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 179000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 180000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 181000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 182000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 183000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 184000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 185000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 186000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 187000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 188000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 189000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 190000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 191000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 192000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 193000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 194000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 195000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 196000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 197000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 198000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 199000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 200000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 201000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 202000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 203000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 204000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 205000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 206000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 207000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 208000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 209000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 210000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 211000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 212000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 213000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 214000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 215000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 216000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 217000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 218000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 219000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 220000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 221000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 222000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 223000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 224000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 225000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 226000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 227000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 228000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 229000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 230000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 231000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 232000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 233000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 234000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 235000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 236000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 237000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 238000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 239000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 240000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 241000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 242000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 243000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 244000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 245000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 246000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 247000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 248000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 249000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 250000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 251000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 252000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 253000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 254000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 255000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 256000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 257000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 258000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 259000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 260000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 261000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 262000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 263000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 264000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 265000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 266000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 267000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 268000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 269000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 270000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 271000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 272000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 273000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 274000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 275000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 276000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 277000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 278000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 279000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 280000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 281000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 282000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 283000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 284000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 285000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 286000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 287000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 288000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 289000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 290000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 291000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 292000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 293000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 294000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 295000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 296000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 297000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 298000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 299000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 300000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 301000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 302000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 303000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 304000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 305000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 306000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 307000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 308000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 309000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 310000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 311000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 312000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 313000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 314000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 315000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 316000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 317000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 318000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 319000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 320000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 321000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 322000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 323000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 324000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 325000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 326000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 327000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 328000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 329000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 330000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 331000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 332000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 333000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 334000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 335000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 336000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 337000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 338000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 339000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 340000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 341000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 342000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 343000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 344000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 345000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 346000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 347000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 348000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 349000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 350000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 351000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 352000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 353000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 354000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 355000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 356000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 357000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 358000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 359000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 360000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 361000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 362000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 363000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 364000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 365000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 366000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 367000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 368000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 369000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 370000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 371000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 372000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 373000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 374000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 375000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 376000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 377000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 378000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Processed 379000 rows so far.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n",
      "Saving final 508 rows.\n",
      "Updated cumulative file to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/cumulative_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3501' max='29650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3501/29650 49:46 < 6:11:55, 1.17 it/s, Epoch 0.59/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m\n\u001b[1;32m     67\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     68\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39ms3_output_dir,\n\u001b[1;32m     69\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     83\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     84\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     85\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     89\u001b[0m )\n\u001b[0;32m---> 91\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     95\u001b[0m final_accuracy \u001b[38;5;241m=\u001b[39m eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3097\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3095\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3097\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3727\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3834\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3832\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3834\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:3029\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 3029\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3031\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/safetensors/torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket_name = \"sagemaker-studio-619071335465-8h7owh9eftx\"\n",
    "s3_file_key = 'training/outputs/cumulative_reviews.csv'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(distilbert_model_name, num_labels=2)\n",
    "\n",
    "def append_to_s3(df, bucket_name, file_key):\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        existing_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        combined_df = df\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    combined_df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    print(f\"Updated cumulative file to s3://{bucket_name}/{file_key}\")\n",
    "\n",
    "\n",
    "def get_last_saved_row(bucket_name, file_key):\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        saved_df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "        return len(saved_df)\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        return 0\n",
    "\n",
    "\n",
    "start_row = get_last_saved_row(bucket_name, s3_file_key)\n",
    "print(f\"Resuming processing from row {start_row}.\")\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "intermittent_data = []\n",
    "processed_dataset = train_dataset[start_row:]\n",
    "\n",
    "for i, (review, label) in enumerate(zip(processed_dataset[\"text\"], processed_dataset[\"label\"]), start=start_row+1):\n",
    "    inputs = tokenizer(review, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    intermittent_data.append({\n",
    "        \"review\": review,\n",
    "        \"true_label\": label,\n",
    "        \"predicted_label\": predicted_label\n",
    "    })\n",
    "\n",
    "    if i % chunk_size == 0:\n",
    "        print(f\"Processed {i} rows so far.\")\n",
    "        intermittent_df = pd.DataFrame(intermittent_data)\n",
    "        append_to_s3(intermittent_df, bucket_name, s3_file_key)\n",
    "        intermittent_data = []\n",
    "\n",
    "\n",
    "if intermittent_data:\n",
    "    print(f\"Saving final {len(intermittent_data)} rows.\")\n",
    "    intermittent_df = pd.DataFrame(intermittent_data)\n",
    "    append_to_s3(intermittent_df, bucket_name, s3_file_key)\n",
    "    \n",
    "\n",
    "s3_output_dir = \"s3://sagemaker-studio-619071335465-8h7owh9eftx/training/outputs/\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=s3_output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=best_learning_rate,\n",
    "    per_device_train_batch_size=best_batch_size,\n",
    "    per_device_eval_batch_size=best_batch_size,\n",
    "    num_train_epochs=best_num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{s3_output_dir}/logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "final_accuracy = eval_results[\"eval_accuracy\"]\n",
    "print(f\"Final evaluation accuracy: {final_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
