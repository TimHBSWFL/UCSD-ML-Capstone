{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.11/site-packages (2024.10.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.11/site-packages (from s3fs) (2.13.3)\n",
      "Requirement already satisfied: fsspec==2024.10.0.* in /opt/conda/lib/python3.11/site-packages (from s3fs) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.34.163,>=1.34.70 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.162)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.15.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.19)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.163,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 04:00:19.805303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-26 04:00:19.825571: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-26 04:00:19.832088: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 04:00:19.847135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import sagemaker\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training/image classification/df_upscale_photos.csv',\n",
       " 'training/image classification/final_image_to_text_results.csv',\n",
       " 'training/image classification/highly_rated.csv',\n",
       " 'training/image classification/lower_rated.csv',\n",
       " 'training/image classification/middle_rated.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"sagemaker-studio-619071335465-8h7owh9eftx\"\n",
    "main_image_dir = 'training/image classification/'\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=main_image_dir)\n",
    "\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended DataFrame shape: (5807, 21)\n",
      "Excluded 'final_image_to_text_results.csv' shape: (17421, 3)\n"
     ]
    }
   ],
   "source": [
    "excluded_file = 'training/image classification/final_image_to_text_results.csv'\n",
    "csv_files_to_append = [file for file in csv_files if file != excluded_file]\n",
    "\n",
    "appended_dataframes = []\n",
    "\n",
    "for file_key in csv_files_to_append:\n",
    "    file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    appended_dataframes.append(df)\n",
    "\n",
    "df_concat = pd.concat(appended_dataframes, ignore_index=True)\n",
    "\n",
    "excluded_file_path = f\"s3://{bucket_name}/{excluded_file}\"\n",
    "df_captions = pd.read_csv(excluded_file_path)\n",
    "\n",
    "\n",
    "print(f\"Appended DataFrame shape: {df_concat.shape}\")\n",
    "print(f\"Excluded 'final_image_to_text_results.csv' shape: {df_captions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7sVrf-VF50HGES_h8OQ46A</td>\n",
       "      <td>fine dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1Zh80DfJ5okYm2S8wxePUA</td>\n",
       "      <td>fine dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jBEueCghl2S_bFDuVux1lA</td>\n",
       "      <td>fine dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO3puIMIwXjNbstSXXXh0A</td>\n",
       "      <td>fine dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K5wG2QbekQxLd8VmE0Hu2A</td>\n",
       "      <td>fine dining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 photo_id        label\n",
       "0  7sVrf-VF50HGES_h8OQ46A  fine dining\n",
       "1  1Zh80DfJ5okYm2S8wxePUA  fine dining\n",
       "2  jBEueCghl2S_bFDuVux1lA  fine dining\n",
       "3  NO3puIMIwXjNbstSXXXh0A  fine dining\n",
       "4  K5wG2QbekQxLd8VmE0Hu2A  fine dining"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sliced = df_concat[['photo_id', 'label']]\n",
    "df_sliced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>nlpconnect/vit-gpt2-image-captioning</td>\n",
       "      <td>a refrigerator with a picture of a pizza on it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-large</td>\n",
       "      <td>there is a large salad bar with a bunch of veg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-base</td>\n",
       "      <td>a kitchen with a large sign above it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0fa0mOVKrJW90MFFxVImg.jpg</td>\n",
       "      <td>nlpconnect/vit-gpt2-image-captioning</td>\n",
       "      <td>a hot dog with mustard and ketchup on a bun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0fa0mOVKrJW90MFFxVImg.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-large</td>\n",
       "      <td>araffe with a pickle and a side of french fries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     photo_id                              model_name  \\\n",
       "0  -0CTxYw82SWnJfzPOBBIOQ.jpg    nlpconnect/vit-gpt2-image-captioning   \n",
       "1  -0CTxYw82SWnJfzPOBBIOQ.jpg  Salesforce/blip-image-captioning-large   \n",
       "2  -0CTxYw82SWnJfzPOBBIOQ.jpg   Salesforce/blip-image-captioning-base   \n",
       "3  -0fa0mOVKrJW90MFFxVImg.jpg    nlpconnect/vit-gpt2-image-captioning   \n",
       "4  -0fa0mOVKrJW90MFFxVImg.jpg  Salesforce/blip-image-captioning-large   \n",
       "\n",
       "                                             caption  \n",
       "0    a refrigerator with a picture of a pizza on it   \n",
       "1  there is a large salad bar with a bunch of veg...  \n",
       "2               a kitchen with a large sign above it  \n",
       "3       a hot dog with mustard and ketchup on a bun   \n",
       "4    araffe with a pickle and a side of french fries  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17421, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_captions['photo_id'] = df_captions['photo_id'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "df_merged = df_captions.merge(df_sliced, how='left', on='photo_id')\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "fast food      15033\n",
       "fine dining     2388\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['photo_id'] = df_merged['photo_id'] + '.jpg'\n",
    "\n",
    "df_merged['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 5807\n",
      "Example image paths:\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-0CTxYw82SWnJfzPOBBIOQ.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-0fa0mOVKrJW90MFFxVImg.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-4PTjFxdyR-tkxDhVeuAfQ.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-6wM47iMcw_wjW3gZYaz-g.jpg\n",
      "image datasets/fastfood images/Fastfood Images/Highly Rated/-7Z1mIroHNK6IJKHMLfnJg.jpg\n"
     ]
    }
   ],
   "source": [
    "image_dir = 'image datasets/'\n",
    "\n",
    "def get_all_images(bucket, prefix):\n",
    "    continuation_token = None\n",
    "    image_keys = []\n",
    "\n",
    "    while True:\n",
    "        list_params = {\n",
    "            'Bucket': bucket,\n",
    "            'Prefix': prefix,\n",
    "        }\n",
    "        if continuation_token:\n",
    "            list_params['ContinuationToken'] = continuation_token\n",
    "\n",
    "        response = s3.list_objects_v2(**list_params)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.jpg'):\n",
    "                    image_keys.append(key)\n",
    "\n",
    "        if not response.get('IsTruncated'):\n",
    "            break\n",
    "\n",
    "        continuation_token = response.get('NextContinuationToken')\n",
    "\n",
    "    return image_keys\n",
    "\n",
    "\n",
    "image_paths = get_all_images(bucket_name, image_dir)\n",
    "\n",
    "\n",
    "if len(image_paths) > 0:\n",
    "    print(f\"Total images found: {len(image_paths)}\")\n",
    "    print(\"Example image paths:\")\n",
    "    for path in image_paths[:5]:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>nlpconnect/vit-gpt2-image-captioning</td>\n",
       "      <td>a refrigerator with a picture of a pizza on it</td>\n",
       "      <td>fast food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-large</td>\n",
       "      <td>there is a large salad bar with a bunch of veg...</td>\n",
       "      <td>fast food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0CTxYw82SWnJfzPOBBIOQ.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-base</td>\n",
       "      <td>a kitchen with a large sign above it</td>\n",
       "      <td>fast food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0fa0mOVKrJW90MFFxVImg.jpg</td>\n",
       "      <td>nlpconnect/vit-gpt2-image-captioning</td>\n",
       "      <td>a hot dog with mustard and ketchup on a bun</td>\n",
       "      <td>fast food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0fa0mOVKrJW90MFFxVImg.jpg</td>\n",
       "      <td>Salesforce/blip-image-captioning-large</td>\n",
       "      <td>araffe with a pickle and a side of french fries</td>\n",
       "      <td>fast food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     photo_id                              model_name  \\\n",
       "0  -0CTxYw82SWnJfzPOBBIOQ.jpg    nlpconnect/vit-gpt2-image-captioning   \n",
       "1  -0CTxYw82SWnJfzPOBBIOQ.jpg  Salesforce/blip-image-captioning-large   \n",
       "2  -0CTxYw82SWnJfzPOBBIOQ.jpg   Salesforce/blip-image-captioning-base   \n",
       "3  -0fa0mOVKrJW90MFFxVImg.jpg    nlpconnect/vit-gpt2-image-captioning   \n",
       "4  -0fa0mOVKrJW90MFFxVImg.jpg  Salesforce/blip-image-captioning-large   \n",
       "\n",
       "                                             caption      label  labels  \n",
       "0    a refrigerator with a picture of a pizza on it   fast food       0  \n",
       "1  there is a large salad bar with a bunch of veg...  fast food       0  \n",
       "2               a kitchen with a large sign above it  fast food       0  \n",
       "3       a hot dog with mustard and ketchup on a bun   fast food       0  \n",
       "4    araffe with a pickle and a side of french fries  fast food       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['labels'] = df_merged['label'].map({\"fast food\": 0, \"fine dining\": 1})\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3686bc123f504206a47cce8eb08c541a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d1d16b1e77436f881ca08e5311dc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9df9cca1ff243da8817f9b1632eb364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41d612961534b29b877a6214d1a1183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75fe19b863a4646988514d7db4c1d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e55d45bc85941e09979f4ea2ae0adc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dafde0207a54877af7486c8132e22a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b1148c2f81497299b8a09c6d6c2191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b49541aa6d44a53b54c33a937a47da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17421, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_sample = df_merged.sample(frac=0.0575, random_state=42)\n",
    "# df_sample.shape\n",
    "\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 17421 images and text features.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_features = []\n",
    "text_features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "def fetch_image_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    image_bytes = response['Body'].read()\n",
    "    return Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "for idx, row in df_merged.iterrows():\n",
    "  target_filename = row[\"photo_id\"] if row[\"photo_id\"].endswith(\".jpg\") else f\"{row['photo_id']}.jpg\"\n",
    "  image_key = next((path for path in image_paths if os.path.basename(path) == target_filename), None)\n",
    "\n",
    "  if image_key:\n",
    "    image = fetch_image_from_s3(bucket_name, image_key)\n",
    "    caption = row[\"caption\"]\n",
    "    label = row[\"labels\"]\n",
    "\n",
    "    inputs = clip_processor(text=[caption], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "      image_embedding = clip_model.get_image_features(pixel_values=inputs['pixel_values']).squeeze().cpu()\n",
    "      text_embedding = clip_model.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).squeeze().cpu()\n",
    "\n",
    "    image_features.append(image_embedding)\n",
    "    text_features.append(text_embedding)\n",
    "    labels.append(label)\n",
    "\n",
    "  else:\n",
    "    print(f\"Image {target_filename} not found in S3.\")\n",
    "\n",
    "\n",
    "image_features = torch.stack(image_features)\n",
    "text_features = torch.stack(text_features)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(f\"Processed {len(image_features)} images and text features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 2, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, image_embedding, text_embedding):\n",
    "        x = torch.cat((image_embedding, text_embedding), dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.000122235845833759, 'batch_size': 8, 'epochs': 15, 'weight_decay': 0.064616307498029, 'dropout_rate': 0.45584099758281393}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.000122235845833759\n",
      "batch_size: 8\n",
      "epochs: 15\n",
      "weight_decay: 0.064616307498029\n",
      "dropout_rate: 0.45584099758281393\n"
     ]
    }
   ],
   "source": [
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "learning_rate = best_params['learning_rate']\n",
    "batch_size = best_params['batch_size']\n",
    "epochs = best_params['epochs']\n",
    "weight_decay = best_params['weight_decay']\n",
    "dropout_rate = best_params['dropout_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25656/4255246848.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(text_train), torch.tensor(y_train))\n",
      "/tmp/ipykernel_25656/4255246848.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(text_val), torch.tensor(y_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Images Processed 100: Loss = 0.3494\n",
      "Images Processed 200: Loss = 0.1392\n",
      "Images Processed 300: Loss = 0.1646\n",
      "Images Processed 400: Loss = 0.2167\n",
      "Images Processed 500: Loss = 0.1347\n",
      "Images Processed 600: Loss = 0.1806\n",
      "Images Processed 700: Loss = 0.2010\n",
      "Images Processed 800: Loss = 0.1603\n",
      "Images Processed 900: Loss = 0.0959\n",
      "Images Processed 1000: Loss = 0.1482\n",
      "Images Processed 1100: Loss = 0.1151\n",
      "Images Processed 1200: Loss = 0.2528\n",
      "Images Processed 1300: Loss = 0.1382\n",
      "Images Processed 1400: Loss = 0.0884\n",
      "Images Processed 1500: Loss = 0.2391\n",
      "Images Processed 1600: Loss = 0.0771\n",
      "Images Processed 1700: Loss = 0.2816\n",
      "Epoch 1\n",
      "Images Processed 100: Loss = 0.1236\n",
      "Images Processed 200: Loss = 0.1021\n",
      "Images Processed 300: Loss = 0.3126\n",
      "Images Processed 400: Loss = 0.1000\n",
      "Images Processed 500: Loss = 0.3029\n",
      "Images Processed 600: Loss = 0.0307\n",
      "Images Processed 700: Loss = 0.2066\n",
      "Images Processed 800: Loss = 0.1394\n",
      "Images Processed 900: Loss = 0.1208\n",
      "Images Processed 1000: Loss = 0.0819\n",
      "Images Processed 1100: Loss = 0.3029\n",
      "Images Processed 1200: Loss = 0.4568\n",
      "Images Processed 1300: Loss = 0.2150\n",
      "Images Processed 1400: Loss = 0.4019\n",
      "Images Processed 1500: Loss = 0.0346\n",
      "Images Processed 1600: Loss = 0.0607\n",
      "Images Processed 1700: Loss = 0.1763\n",
      "Epoch 2\n",
      "Images Processed 100: Loss = 0.1480\n",
      "Images Processed 200: Loss = 0.2524\n",
      "Images Processed 300: Loss = 0.1592\n",
      "Images Processed 400: Loss = 0.0583\n",
      "Images Processed 500: Loss = 0.2778\n",
      "Images Processed 600: Loss = 0.1028\n",
      "Images Processed 700: Loss = 0.1561\n",
      "Images Processed 800: Loss = 0.2409\n",
      "Images Processed 900: Loss = 0.0501\n",
      "Images Processed 1000: Loss = 0.1577\n",
      "Images Processed 1100: Loss = 0.0737\n",
      "Images Processed 1200: Loss = 0.2004\n",
      "Images Processed 1300: Loss = 0.3473\n",
      "Images Processed 1400: Loss = 0.3545\n",
      "Images Processed 1500: Loss = 0.3987\n",
      "Images Processed 1600: Loss = 0.1334\n",
      "Images Processed 1700: Loss = 0.3663\n",
      "Epoch 3\n",
      "Images Processed 100: Loss = 0.2826\n",
      "Images Processed 200: Loss = 0.1190\n",
      "Images Processed 300: Loss = 0.2354\n",
      "Images Processed 400: Loss = 0.3561\n",
      "Images Processed 500: Loss = 0.0818\n",
      "Images Processed 600: Loss = 0.2842\n",
      "Images Processed 700: Loss = 0.2136\n",
      "Images Processed 800: Loss = 0.0900\n",
      "Images Processed 900: Loss = 0.0497\n",
      "Images Processed 1000: Loss = 0.1099\n",
      "Images Processed 1100: Loss = 0.1454\n",
      "Images Processed 1200: Loss = 0.3723\n",
      "Images Processed 1300: Loss = 0.1645\n",
      "Images Processed 1400: Loss = 0.2726\n",
      "Images Processed 1500: Loss = 0.3013\n",
      "Images Processed 1600: Loss = 0.2816\n",
      "Images Processed 1700: Loss = 0.4282\n",
      "Epoch 4\n",
      "Images Processed 100: Loss = 0.1413\n",
      "Images Processed 200: Loss = 0.0873\n",
      "Images Processed 300: Loss = 0.0609\n",
      "Images Processed 400: Loss = 0.1146\n",
      "Images Processed 500: Loss = 0.1684\n",
      "Images Processed 600: Loss = 0.1407\n",
      "Images Processed 700: Loss = 0.4977\n",
      "Images Processed 800: Loss = 0.1331\n",
      "Images Processed 900: Loss = 0.1568\n",
      "Images Processed 1000: Loss = 0.0991\n",
      "Images Processed 1100: Loss = 0.0488\n",
      "Images Processed 1200: Loss = 0.2211\n",
      "Images Processed 1300: Loss = 0.2865\n",
      "Images Processed 1400: Loss = 0.0818\n",
      "Images Processed 1500: Loss = 0.0607\n",
      "Images Processed 1600: Loss = 0.1582\n",
      "Images Processed 1700: Loss = 0.2153\n",
      "Epoch 5\n",
      "Images Processed 100: Loss = 0.2874\n",
      "Images Processed 200: Loss = 0.0637\n",
      "Images Processed 300: Loss = 0.0236\n",
      "Images Processed 400: Loss = 0.1879\n",
      "Images Processed 500: Loss = 0.2227\n",
      "Images Processed 600: Loss = 0.1484\n",
      "Images Processed 700: Loss = 0.0527\n",
      "Images Processed 800: Loss = 0.3304\n",
      "Images Processed 900: Loss = 0.1302\n",
      "Images Processed 1000: Loss = 0.1274\n",
      "Images Processed 1100: Loss = 0.2120\n",
      "Images Processed 1200: Loss = 0.4446\n",
      "Images Processed 1300: Loss = 0.4059\n",
      "Images Processed 1400: Loss = 0.0550\n",
      "Images Processed 1500: Loss = 0.0403\n",
      "Images Processed 1600: Loss = 0.2845\n",
      "Images Processed 1700: Loss = 0.1036\n",
      "Epoch 6\n",
      "Images Processed 100: Loss = 0.1024\n",
      "Images Processed 200: Loss = 0.1356\n",
      "Images Processed 300: Loss = 0.1796\n",
      "Images Processed 400: Loss = 0.2520\n",
      "Images Processed 500: Loss = 0.1164\n",
      "Images Processed 600: Loss = 0.0444\n",
      "Images Processed 700: Loss = 0.4866\n",
      "Images Processed 800: Loss = 0.2608\n",
      "Images Processed 900: Loss = 0.2840\n",
      "Images Processed 1000: Loss = 0.3134\n",
      "Images Processed 1100: Loss = 0.1589\n",
      "Images Processed 1200: Loss = 0.0764\n",
      "Images Processed 1300: Loss = 0.1199\n",
      "Images Processed 1400: Loss = 0.1291\n",
      "Images Processed 1500: Loss = 0.1725\n",
      "Images Processed 1600: Loss = 0.1295\n",
      "Images Processed 1700: Loss = 0.0825\n",
      "Epoch 7\n",
      "Images Processed 100: Loss = 0.0856\n",
      "Images Processed 200: Loss = 0.3437\n",
      "Images Processed 300: Loss = 0.2271\n",
      "Images Processed 400: Loss = 0.3201\n",
      "Images Processed 500: Loss = 0.1124\n",
      "Images Processed 600: Loss = 0.4496\n",
      "Images Processed 700: Loss = 0.1101\n",
      "Images Processed 800: Loss = 0.2036\n",
      "Images Processed 900: Loss = 0.2104\n",
      "Images Processed 1000: Loss = 0.2788\n",
      "Images Processed 1100: Loss = 0.6318\n",
      "Images Processed 1200: Loss = 0.0933\n",
      "Images Processed 1300: Loss = 0.2952\n",
      "Images Processed 1400: Loss = 0.1031\n",
      "Images Processed 1500: Loss = 0.0307\n",
      "Images Processed 1600: Loss = 0.3978\n",
      "Images Processed 1700: Loss = 0.4055\n",
      "Epoch 8\n",
      "Images Processed 100: Loss = 0.2265\n",
      "Images Processed 200: Loss = 0.0792\n",
      "Images Processed 300: Loss = 0.1263\n",
      "Images Processed 400: Loss = 0.0243\n",
      "Images Processed 500: Loss = 0.1555\n",
      "Images Processed 600: Loss = 0.1248\n",
      "Images Processed 700: Loss = 0.2536\n",
      "Images Processed 800: Loss = 0.1477\n",
      "Images Processed 900: Loss = 0.2126\n",
      "Images Processed 1000: Loss = 0.0387\n",
      "Images Processed 1100: Loss = 0.4776\n",
      "Images Processed 1200: Loss = 0.0498\n",
      "Images Processed 1300: Loss = 0.0705\n",
      "Images Processed 1400: Loss = 0.3634\n",
      "Images Processed 1500: Loss = 0.1282\n",
      "Images Processed 1600: Loss = 0.0867\n",
      "Images Processed 1700: Loss = 0.0845\n",
      "Epoch 9\n",
      "Images Processed 100: Loss = 0.1753\n",
      "Images Processed 200: Loss = 0.0383\n",
      "Images Processed 300: Loss = 0.2712\n",
      "Images Processed 400: Loss = 0.1836\n",
      "Images Processed 500: Loss = 0.4577\n",
      "Images Processed 600: Loss = 0.0969\n",
      "Images Processed 700: Loss = 0.4035\n",
      "Images Processed 800: Loss = 0.2414\n",
      "Images Processed 900: Loss = 0.1759\n",
      "Images Processed 1000: Loss = 0.1306\n",
      "Images Processed 1100: Loss = 0.3521\n",
      "Images Processed 1200: Loss = 0.4721\n",
      "Images Processed 1300: Loss = 0.0192\n",
      "Images Processed 1400: Loss = 0.1264\n",
      "Images Processed 1500: Loss = 0.1837\n",
      "Images Processed 1600: Loss = 0.1419\n",
      "Images Processed 1700: Loss = 0.1049\n",
      "Epoch 10\n",
      "Images Processed 100: Loss = 0.1185\n",
      "Images Processed 200: Loss = 0.0668\n",
      "Images Processed 300: Loss = 0.0560\n",
      "Images Processed 400: Loss = 0.1765\n",
      "Images Processed 500: Loss = 0.0648\n",
      "Images Processed 600: Loss = 0.2039\n",
      "Images Processed 700: Loss = 0.3248\n",
      "Images Processed 800: Loss = 0.1985\n",
      "Images Processed 900: Loss = 0.1336\n",
      "Images Processed 1000: Loss = 0.2542\n",
      "Images Processed 1100: Loss = 0.2288\n",
      "Images Processed 1200: Loss = 0.1058\n",
      "Images Processed 1300: Loss = 0.4143\n",
      "Images Processed 1400: Loss = 0.1545\n",
      "Images Processed 1500: Loss = 0.0747\n",
      "Images Processed 1600: Loss = 0.1245\n",
      "Images Processed 1700: Loss = 0.1190\n",
      "Epoch 11\n",
      "Images Processed 100: Loss = 0.0748\n",
      "Images Processed 200: Loss = 0.1934\n",
      "Images Processed 300: Loss = 0.1260\n",
      "Images Processed 400: Loss = 0.1824\n",
      "Images Processed 500: Loss = 0.2060\n",
      "Images Processed 600: Loss = 0.3300\n",
      "Images Processed 700: Loss = 0.2743\n",
      "Images Processed 800: Loss = 0.1157\n",
      "Images Processed 900: Loss = 0.1440\n",
      "Images Processed 1000: Loss = 0.2064\n",
      "Images Processed 1100: Loss = 0.1073\n",
      "Images Processed 1200: Loss = 0.0509\n",
      "Images Processed 1300: Loss = 0.2718\n",
      "Images Processed 1400: Loss = 0.1270\n",
      "Images Processed 1500: Loss = 0.5384\n",
      "Images Processed 1600: Loss = 0.2489\n",
      "Images Processed 1700: Loss = 0.0592\n",
      "Epoch 12\n",
      "Images Processed 100: Loss = 0.3139\n",
      "Images Processed 200: Loss = 0.0524\n",
      "Images Processed 300: Loss = 0.2164\n",
      "Images Processed 400: Loss = 0.0710\n",
      "Images Processed 500: Loss = 0.1443\n",
      "Images Processed 600: Loss = 0.1011\n",
      "Images Processed 700: Loss = 0.0279\n",
      "Images Processed 800: Loss = 0.3508\n",
      "Images Processed 900: Loss = 0.1848\n",
      "Images Processed 1000: Loss = 0.3317\n",
      "Images Processed 1100: Loss = 0.0643\n",
      "Images Processed 1200: Loss = 0.1759\n",
      "Images Processed 1300: Loss = 0.3280\n",
      "Images Processed 1400: Loss = 0.1533\n",
      "Images Processed 1500: Loss = 0.1608\n",
      "Images Processed 1600: Loss = 0.1484\n",
      "Images Processed 1700: Loss = 0.1569\n",
      "Epoch 13\n",
      "Images Processed 100: Loss = 0.0861\n",
      "Images Processed 200: Loss = 0.0715\n",
      "Images Processed 300: Loss = 0.3657\n",
      "Images Processed 400: Loss = 0.0496\n",
      "Images Processed 500: Loss = 0.0754\n",
      "Images Processed 600: Loss = 0.4012\n",
      "Images Processed 700: Loss = 0.1896\n",
      "Images Processed 800: Loss = 0.0381\n",
      "Images Processed 900: Loss = 0.2087\n",
      "Images Processed 1000: Loss = 0.1281\n",
      "Images Processed 1100: Loss = 0.2547\n",
      "Images Processed 1200: Loss = 0.0613\n",
      "Images Processed 1300: Loss = 0.1597\n",
      "Images Processed 1400: Loss = 0.0954\n",
      "Images Processed 1500: Loss = 0.2648\n",
      "Images Processed 1600: Loss = 0.1372\n",
      "Images Processed 1700: Loss = 0.2546\n",
      "Epoch 14\n",
      "Images Processed 100: Loss = 0.0517\n",
      "Images Processed 200: Loss = 0.0455\n",
      "Images Processed 300: Loss = 0.0947\n",
      "Images Processed 400: Loss = 0.0678\n",
      "Images Processed 500: Loss = 0.1981\n",
      "Images Processed 600: Loss = 0.0589\n",
      "Images Processed 700: Loss = 0.1276\n",
      "Images Processed 800: Loss = 0.1523\n",
      "Images Processed 900: Loss = 0.1469\n",
      "Images Processed 1000: Loss = 0.1581\n",
      "Images Processed 1100: Loss = 0.1373\n",
      "Images Processed 1200: Loss = 0.0763\n",
      "Images Processed 1300: Loss = 0.0388\n",
      "Images Processed 1400: Loss = 0.1706\n",
      "Images Processed 1500: Loss = 0.3786\n",
      "Images Processed 1600: Loss = 0.1191\n",
      "Images Processed 1700: Loss = 0.0780\n",
      "Validation Accuracy: 0.939167862266858\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(image_features, labels, test_size=0.2, random_state=42)\n",
    "text_train, text_val = train_test_split(text_features, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(text_train), torch.tensor(y_train))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(text_val), torch.tensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "input_dim = image_features.shape[1]\n",
    "hidden_dim = 256\n",
    "model = MultimodalClassifier(input_dim=input_dim, hidden_dim=hidden_dim, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "\n",
    "    model.train()\n",
    "    for i, (image_batch, text_batch, label_batch) in enumerate(train_loader, start=1):\n",
    "        image_batch = image_batch.to(device)\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image_batch, text_batch).squeeze(-1)\n",
    "        loss = criterion(output, label_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Images Processed {i}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image_batch, text_batch, label_batch in val_loader:\n",
    "        image_batch = image_batch.to(device)\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        output = model(image_batch, text_batch)\n",
    "        val_predictions.extend(output.cpu().numpy())\n",
    "        val_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "val_predictions = np.array(val_predictions) > 0.5\n",
    "val_acc = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state saved to multimodal_classifier_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"multimodal_classifier_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model state saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to s3://sagemaker-studio-619071335465-8h7owh9eftx/training/models/multimodal_classifier_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s3_bucket = \"sagemaker-studio-619071335465-8h7owh9eftx\"\n",
    "s3_key = \"training/models/multimodal_classifier_model.pth\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "s3.upload_file(model_save_path, s3_bucket, s3_key)\n",
    "print(f\"Model uploaded to s3://{s3_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-studio-619071335465-8h7owh9eftx/training/models/multimodal_classifier_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_artifact = f\"s3://{s3_bucket}/{s3_key}\"\n",
    "print(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[ 4.1057e-03,  8.4955e-04, -3.3628e-03,  ..., -2.6028e-03,\n",
      "          7.6136e-04, -2.7815e-04],\n",
      "        [-2.7187e-03, -1.8087e-03,  2.2379e-03,  ...,  4.7140e-03,\n",
      "          1.0096e-03,  1.6289e-04],\n",
      "        [ 3.5197e-03,  5.2734e-03, -6.2515e-03,  ..., -2.2891e-03,\n",
      "         -1.6100e-03, -2.6531e-03],\n",
      "        ...,\n",
      "        [ 5.7256e-03,  3.6784e-03, -5.2710e-03,  ..., -3.3767e-03,\n",
      "         -1.5594e-04, -2.4984e-03],\n",
      "        [-3.9317e-03, -5.1555e-05,  4.2796e-03,  ...,  2.0974e-03,\n",
      "         -7.9344e-04,  6.9403e-04],\n",
      "        [ 9.2486e-44, -8.5479e-44, -7.9874e-44,  ..., -4.6243e-44,\n",
      "          8.8282e-44,  9.6690e-44]], device='cuda:0')), ('fc1.bias', tensor([ 6.6227e-03, -5.1327e-04,  6.2926e-03,  2.0024e-20,  4.5280e-03,\n",
      "         5.4196e-03,  6.8448e-03, -2.5870e-17,  6.0964e-03,  3.3327e-03,\n",
      "         5.8031e-03, -1.2752e-03,  4.5492e-03, -1.6060e-03,  1.1419e-05,\n",
      "         5.7791e-03,  3.4429e-03,  4.7352e-03, -1.9316e-20, -3.1827e-04,\n",
      "         5.1584e-03, -1.2274e-03, -1.2458e-03, -5.1486e-17, -4.2039e-44,\n",
      "         1.7170e-20, -7.8473e-44,  5.4703e-03, -3.0712e-04,  6.4722e-11,\n",
      "        -7.9874e-44, -7.9874e-44,  2.7546e-03, -6.2217e-04,  5.7192e-03,\n",
      "         1.2292e-03,  1.4863e-10,  3.9519e-03, -2.5846e-05,  4.0305e-03,\n",
      "         4.6382e-03, -8.1275e-44,  4.2144e-03,  4.9919e-03, -5.7789e-05,\n",
      "         2.3883e-03, -6.7985e-04,  5.2187e-03, -4.2311e-14,  5.6202e-03,\n",
      "         6.6962e-03,  9.0571e-17, -7.8473e-44,  2.9500e-03,  3.6539e-03,\n",
      "         6.3006e-03,  1.3473e-03,  3.7095e-03,  5.7182e-03,  5.0876e-03,\n",
      "        -2.8026e-45, -8.4078e-44, -1.4644e-03, -9.5421e-17,  3.9314e-03,\n",
      "        -4.0638e-44, -3.1553e-07, -1.9646e-33,  7.7071e-44,  2.1926e-04,\n",
      "         5.2865e-03,  5.5882e-03, -1.7798e-03, -3.3395e-23,  5.4242e-03,\n",
      "        -7.8473e-44, -8.6452e-04,  5.0827e-03, -8.5173e-22, -9.5288e-44,\n",
      "         1.4071e-03,  4.4104e-03,  4.0551e-03,  5.1674e-03,  5.3760e-03,\n",
      "        -6.2552e-04, -1.4564e-13,  6.5309e-03, -4.0704e-16,  3.6467e-03,\n",
      "         6.7513e-03,  3.0787e-03,  4.5186e-03,  4.0812e-03, -8.3504e-22,\n",
      "         6.8202e-04, -3.0829e-44,  4.6543e-03, -1.4195e-03,  3.2494e-03,\n",
      "         6.2212e-03,  5.9491e-03,  2.1619e-03,  3.7654e-13,  3.6691e-04,\n",
      "        -7.8473e-44,  4.5635e-04,  3.9160e-03, -1.2735e-07, -1.0180e-03,\n",
      "         5.0680e-03, -2.0294e-06,  2.3403e-03, -6.6574e-19,  5.1226e-03,\n",
      "        -9.5863e-08,  3.6851e-03,  3.3211e-03,  3.3571e-05, -7.7071e-44,\n",
      "        -7.7071e-44,  3.3254e-04, -7.1257e-04,  2.9324e-03,  4.3440e-44,\n",
      "        -1.9078e-03,  4.7527e-03,  4.4916e-03,  4.9560e-03, -2.3105e-10,\n",
      "         7.5619e-04, -8.9416e-04, -8.1275e-44,  5.9454e-03, -3.1165e-08,\n",
      "        -7.7071e-44,  3.6017e-03,  5.8916e-03,  2.4435e-30,  1.7507e-20,\n",
      "        -7.7071e-44, -7.8473e-44,  7.6613e-03,  3.0139e-32, -8.1275e-44,\n",
      "         4.0481e-08,  5.7178e-03,  7.4354e-03, -7.8473e-44,  4.3052e-03,\n",
      "         3.0642e-03, -1.5473e-03, -8.1275e-44, -7.8473e-44,  4.1105e-03,\n",
      "         7.2666e-03,  3.0401e-03, -9.1084e-44, -1.0666e-03, -2.2122e-06,\n",
      "         4.9869e-03,  2.1728e-03,  4.4798e-03,  6.4995e-04,  4.7801e-03,\n",
      "         5.3293e-03, -2.8603e-08,  5.5378e-03,  4.4959e-03, -7.8473e-44,\n",
      "        -3.0257e-04,  3.2953e-03,  4.0299e-03, -8.1275e-44, -7.8473e-44,\n",
      "         7.1857e-03, -9.7013e-04,  1.1870e-03,  8.3556e-03, -4.6728e-05,\n",
      "        -7.0065e-45,  4.4192e-03, -7.4066e-04, -6.2645e-04,  6.1995e-03,\n",
      "        -2.0915e-04, -7.8473e-44, -7.8473e-44,  3.2234e-03,  6.3074e-03,\n",
      "        -2.4681e-07,  9.8593e-04, -8.1275e-44,  4.6202e-03, -7.9874e-44,\n",
      "         1.3083e-04,  4.1073e-03, -7.8473e-44,  3.8698e-03,  1.9012e-04,\n",
      "        -1.1392e-03,  3.9057e-03,  3.1105e-03,  4.1451e-03,  5.8091e-03,\n",
      "         3.7214e-03, -1.2732e-03,  6.1460e-04, -7.0065e-44, -4.0946e-04,\n",
      "         4.8076e-03,  3.2065e-03, -1.5877e-03,  5.0784e-04,  7.0838e-04,\n",
      "         5.4041e-03,  5.4691e-03, -1.1816e-03,  7.5748e-03, -8.1275e-44,\n",
      "         3.9236e-44,  2.9427e-44, -1.8079e-03,  7.7071e-44, -7.9874e-44,\n",
      "        -6.8302e-04, -7.7071e-44, -7.8473e-44,  5.3315e-03,  3.6958e-04,\n",
      "        -7.9874e-44,  4.4960e-03,  1.7113e-09,  5.5375e-03, -3.6811e-10,\n",
      "         6.0636e-03,  1.5670e-03, -2.2325e-18, -8.4078e-44, -7.7071e-44,\n",
      "        -9.0113e-04, -2.0323e-03, -7.0065e-45,  1.5304e-09, -4.6716e-18,\n",
      "         3.3157e-03,  4.4348e-03,  4.8238e-03,  7.9086e-03,  2.8884e-03,\n",
      "        -1.3732e-03,  3.7380e-03,  4.1860e-03,  7.3367e-03, -1.3538e-03,\n",
      "        -7.9874e-44], device='cuda:0')), ('fc2.weight', tensor([[-9.2290e-02,  7.8550e-02, -1.0326e-01,  9.8694e-20, -1.1519e-01,\n",
      "         -1.0135e-01, -1.1149e-01, -9.4784e-16, -8.6607e-02, -3.2183e-02,\n",
      "         -9.5188e-02,  9.5973e-02, -8.3090e-02,  8.9477e-02, -1.7473e-05,\n",
      "         -9.2389e-02, -5.7870e-02, -8.4497e-02, -2.7200e-20,  9.3109e-02,\n",
      "         -1.0946e-01,  8.0058e-02,  5.9261e-02,  3.1818e-16, -7.7071e-44,\n",
      "         -4.6213e-20, -5.4651e-44, -9.5332e-02,  1.0929e-01,  1.1157e-08,\n",
      "          7.8473e-44,  7.8473e-44, -5.3811e-02,  6.3118e-02, -1.1893e-01,\n",
      "         -2.4116e-02,  6.8883e-11, -7.6281e-02,  9.0460e-02, -9.4318e-02,\n",
      "         -9.3344e-02,  7.7071e-44, -9.1204e-02, -6.6589e-02,  7.7995e-02,\n",
      "         -4.6554e-02,  8.2491e-02, -9.7015e-02, -7.2797e-13, -1.1076e-01,\n",
      "         -9.8323e-02, -5.2447e-16,  7.7071e-44, -5.9190e-02, -8.6593e-02,\n",
      "         -1.1417e-01,  5.0785e-02, -8.0169e-02, -1.0628e-01, -8.5536e-02,\n",
      "          7.8473e-44, -7.7071e-44,  9.8501e-02,  1.9890e-15, -1.0251e-01,\n",
      "         -7.7071e-44,  9.9272e-06,  6.1663e-07, -9.3139e-24, -1.0650e-03,\n",
      "         -1.0470e-01, -9.3527e-02,  7.8813e-02,  7.6150e-23, -9.7970e-02,\n",
      "         -7.7071e-44,  5.8199e-02, -1.0237e-01,  2.6513e-21,  7.7071e-44,\n",
      "          8.3118e-02, -8.4802e-02, -7.5312e-02, -1.1380e-01, -7.6661e-02,\n",
      "          8.3750e-02,  1.2496e-12, -1.0394e-01,  1.7629e-15, -7.6808e-02,\n",
      "         -1.1579e-01, -4.5846e-02, -9.1928e-02, -7.1286e-02,  1.1605e-20,\n",
      "          1.0149e-01, -7.7071e-44, -9.5168e-02,  9.6930e-02, -6.9404e-02,\n",
      "         -9.3249e-02, -1.1089e-01, -3.5227e-02,  3.6941e-10,  7.3280e-02,\n",
      "          7.7071e-44,  9.2638e-02, -7.1101e-02,  5.8582e-08,  9.2468e-02,\n",
      "         -1.0800e-01,  2.9077e-05, -2.1788e-02, -1.0100e-18, -9.2355e-02,\n",
      "          3.2350e-07, -8.5526e-02, -7.2064e-02,  3.6913e-02, -7.7071e-44,\n",
      "         -7.7071e-44,  6.3212e-02,  5.6831e-02, -7.2599e-02,  7.7071e-44,\n",
      "          1.0456e-01, -1.0220e-01, -8.1976e-02, -1.0240e-01, -5.9854e-11,\n",
      "          9.2727e-02,  5.5197e-02, -7.7071e-44, -1.0729e-01, -6.8581e-08,\n",
      "         -7.7071e-44, -9.1119e-02, -9.4849e-02,  1.6815e-29, -8.4200e-20,\n",
      "          7.7071e-44, -7.7071e-44, -1.0968e-01,  4.9002e-31, -7.8473e-44,\n",
      "          7.4978e-09, -9.7562e-02, -1.0055e-01,  7.7071e-44, -5.4284e-02,\n",
      "         -6.9652e-02,  9.2817e-02,  7.7071e-44,  7.7071e-44, -8.0447e-02,\n",
      "         -1.0240e-01, -6.1931e-02, -7.8473e-44,  9.7161e-02, -1.6005e-07,\n",
      "         -8.9872e-02,  8.7979e-02, -8.3786e-02,  3.6897e-02, -8.9329e-02,\n",
      "         -9.8979e-02,  2.2817e-07, -9.7315e-02, -1.0289e-01,  7.8473e-44,\n",
      "          8.0877e-02, -5.3698e-02, -7.2802e-02,  7.7071e-44, -7.7071e-44,\n",
      "         -1.0029e-01,  6.2921e-02,  7.3603e-02, -1.0950e-01,  8.9653e-02,\n",
      "          7.7071e-44, -9.2497e-02,  1.0976e-01,  9.1528e-02, -1.2266e-01,\n",
      "          2.9876e-04, -7.7071e-44, -7.7071e-44, -4.5908e-02, -1.0358e-01,\n",
      "         -2.5628e-05,  1.0183e-01,  7.7071e-44, -9.6691e-02, -7.7071e-44,\n",
      "          1.0200e-01, -8.9313e-02, -7.8473e-44, -7.7200e-02,  7.2882e-02,\n",
      "          9.5257e-02, -1.0229e-01, -7.1676e-02, -8.2654e-02, -1.1317e-01,\n",
      "         -8.6684e-02,  8.6963e-02, -6.3915e-03,  7.8473e-44,  1.1508e-01,\n",
      "         -1.0248e-01, -5.8775e-02,  5.3342e-02,  9.5084e-02,  9.7686e-02,\n",
      "         -1.0256e-01, -1.0089e-01,  6.7501e-02, -1.1463e-01,  7.7071e-44,\n",
      "          7.7071e-44, -7.7071e-44,  8.7920e-02, -7.7071e-44,  7.7071e-44,\n",
      "          3.4103e-02, -7.7071e-44,  7.7071e-44, -1.2320e-01,  9.5419e-02,\n",
      "         -7.7071e-44, -6.5799e-02, -7.9352e-09, -1.0073e-01,  7.5798e-08,\n",
      "         -8.8570e-02, -2.6162e-02, -1.7521e-18,  7.7071e-44, -7.7071e-44,\n",
      "          1.0413e-01,  7.0430e-02, -7.7071e-44,  2.7406e-09,  3.4522e-18,\n",
      "         -7.5296e-02, -8.0805e-02, -1.0949e-01, -1.2985e-01, -5.9865e-02,\n",
      "          1.0907e-01, -8.6181e-02, -9.5146e-02, -1.3382e-01,  9.9728e-02,\n",
      "          7.7071e-44]], device='cuda:0')), ('fc2.bias', tensor([-0.0217], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "model_path = 'multimodal_classifier_model.pth'\n",
    "state_dict = torch.load(model_path)\n",
    "print(state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
